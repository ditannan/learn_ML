---
title: "模型选择"
output: html_notebook
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(comment = '#>')
Sys.setlocale('LC_ALL','C')
```


# 最优子集选择、向前向后选择，验证集和交叉验证法。

```{r loading packages}
library(ISLR)
library(leaps)
library(magrittr)
```

```{r}
names(Hitters)
dim(Hitters)
```

```{r}
head(Hitters)
```

```{r}
sum(is.na(Hitters$Salary))
```

删除缺失
```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
```

`leaps`包中`regsubset()`函数可实现最优预测变量子集的筛选
```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
reg.summary
```

```{r}
names(reg.summary)
reg.summary$adjr2
```

```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = 'Number of Variables', ylab = 'RSS', type = 'l')
plot(reg.summary$adjr2, xlab = 'Number of Variables', ylab = 'Adjusted Rsq', type = 'l')
# 标出最大值对应的位置
points(11, reg.summary$adjr2[which.max(reg.summary$adjr2)], col = 'red', pch = 20)
```

内置plot命令
```{r}
plot(regfit.full, scale = 'adjr2')
plot(regfit.full, scale = 'bic')

```

提取参数
```{r}
coef(regfit.full, 6)
```

## 向前逐步选择和向后逐步选择
```{r}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = 'forward')
summary(regfit.fwd)
```
最优单变量只包含CRBI。

```{r}
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = 'backward')
summary(regfit.bwd)
```

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

7变量选择是不同的。

## 使用验证集方法
```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test <- !train
```

在训练集上完成子集选择
```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)
```

看看测试集误差，先生成一个回归设计矩阵
```{r}
test.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])
test.mat %>% str()
```

```{r}
val.errors <- rep(NA, 19)
for (i in 1 : 19) {
  coefi <- coef(regfit.best, i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val.errors
which.min(val.errors)
```

```{r}
plot(sqrt(val.errors), ylab = 'Root MSE', ylim = c(370, 480), pch = 19)
```


```{r}
coef(regfit.best, 10)
```

再对整个数据集选择10个变量的模型
```{r}
coef(regsubsets(Salary ~ ., data = Hitters, nvmax = 19), 10)
```


## 使用交叉验证
定义一个向量将数据集中的每个观测归为k=10折中的某一折
```{r}
k <- 10
set.seed(1)
folds <- sample(1 : k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(nrow = k, ncol = 19, dimnames = list(NULL, paste(1 : 19)) )
cv.errors
```

在第j折中，j折中的观测作为测试集，其他的9折作为训练集。，计算在测试集中的测试误差。

定义预测函数
```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvar <- names(coefi)
  mat[, xvar] %*% coefi
}
```

```{r}
for (j in 1 : k) {
  best.fit <- regsubsets(Salary ~ ., data = Hitters[folds != j, ], nvmax = 19)
  for (i in 1 : 19) {
    pred <- predict.regsubsets(best.fit, newdata = Hitters[folds == j, ], id =i)
    cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)
  }
}
```

```{r}
cv.errors
```

列平均
```{r}
cv.errors.mean <- apply(cv.errors, 2, mean)
cv.errors.mean
```

```{r}
plot(cv.errors.mean, type = 'b')
points(which.min(cv.errors.mean), cv.errors.mean[which.min(cv.errors.mean)], col = 'red')
```

选择了`r which.min(cv.errors.mean)`个变量模型。

使用全部数据或者11个变量模型参数：
```{r}
coef(regsubsets(Salary ~ ., data = Hitters, nvmax = 19), 11)
```


# 岭回归和lasso
```{r}
library(glmnet)
```

构造回归设计矩阵：
```{r}
x <- model.matrix(Salary ~ ., data = Hitters)[, -1]
y <- Hitters$Salary
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
grid <- 10 ^ seq(10, -2, length = 100)
# alpha = 0, 岭回归
ridge.mod <- glmnet(x, y, alpha = 0) 
```
glmnet 函数默认设置下将所有的变量进行了标准化，使用`standardize = FALSE`可关闭该设置。参数$\lambda$每个值对应岭回归的系数向量，可通过函数`coef()`提取系数矩阵。
```{r}
dim(coef(ridge.mod))
```

```{r}
ridge.mod$lambda
```

$\lambda$ = `r ridge.mod$lambda[50]`时系数估计。
```{r}
coef(ridge.mod)[, 50]
```

可通过`predict()`函数获得新的$s=\lambda$对应的岭回归系数：
```{r}
predict(ridge.mod, s = ridge.mod$lambda[50], type = 'coefficients')[1 : 20,]
```

可以看到和上面结果是一样的。
```{r}
plot(ridge.mod, xvar = 'lambda', label = TRUE)
```
```{r}
cv.ridge <- cv.glmnet(x, y, alpha = 0)
plot(cv.ridge)
```

## 分为测试集和训练集
```{r}
set.seed(1)
train <- sample(1 : nrow(x), nrow(x) / 2)
test <- -train
y.test <- y[test]
```

$\lambda=4$时的MSE
```{r}
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# 测试集上预测值
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test) ^ 2)
```

只含截距项时MSE，可通过设置很大的$\lambda$求得：
```{r}
ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])
mean((ridge.pred - y.test) ^ 2)
```

也可：如果只含截距项，模型对测试集的预测就为训练集数据的均值，则MSE：
```{r}
mean((mean(y[train]) - y.test) ^ 2)
```

可以看到和上面结果是一样的。

### 和最小二乘相比
最小二乘估计就是$\lambda=0$时的岭回归。
```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ])
mean((ridge.pred - y.test) ^ 2)
```

还是比$\lambda=4$时的MSE大。
```{r}
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, type = 'coefficients')[1 : 20,]
```

可以看到两者得到的参数估计一致的。

### 使用交叉验证得到最佳 $\lambda$
函数`cv.glmnet()`可用来选择参数，默认十折交叉验证：
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
```

```{r}
bestlam <- cv.out$lambda.min
bestlam
```

看看最佳的$\lambda$对应的MSE是多少：
```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test) ^ 2)
```

基于整个数据集对应的参数估计：
```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, s = bestlam, type = 'coefficients')[1 : 20,]
```

可以看到没有变量的系数为0.

## lasso
只需将$\lambda$设定为1.
```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

可以看到有些系数变为了0.
### 进行交叉验证
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
```

```{r}
bestlam <- cv.out$lambda.min
bestlam
```

对应的MSE
```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test) ^ 2)
```

该值小于最小二乘估计所得到的MSE，与岭回归交叉验证得到的很接近。但可以得到模型的系数估计中，值保留了7个变量：
```{r}
out <- glmnet(x, y, alpha = 1)
lasso.coef <- predict(out, alpha = 1, type = 'coefficients', s = bestlam)[1 : 20,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef != 0]
```

保留了7个变量。

# 主成分回归
可使用pls库中的`pcr()`函数实现主成分回归。
```{r}
library(pls)
```

```{r}
set.seed(2)
# 拟合前标化并使用十折交叉验证
pcr.mod <- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.mod)
```

RMSEP是均方根误差，MSE是其平方。
TRAINING部分是指能提取的响应变量的方差（信息量）。

```{r}
validationplot(pcr.mod, val.type = 'MSEP')
```



## 划分训练测试集
```{r}
set.seed(1)
pcr.mod <- pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = 'CV')
validationplot(pcr.mod, val.type = 'MSEP')
```

```{r}
summary(pcr.mod)
```

可以看到7个主成分时误差最小，计算测试集误差
```{r}
pcr.pred <- predict(pcr.mod, x[test, ], ncomp = 7)
mean((pcr.pred - y.test) ^ 2)
```

最后，在整个数据集上用7个主成分拟合：
```{r}
pcr.mod <- pcr(y ~ x, scale = TRUE, ncomp = 7)
summary(pcr.mod)
```

## 偏最小二乘估计
函数`plsr()`可以拟合最小二乘回归模型
```{r}
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = 'CV')
summary(pls.fit)
```

当M=2个偏最小二乘方向时，交叉验证误差最小。计算相应的测试集MSE
```{r}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 2)
mean((pls.pred - y.test) ^ 2)
```

在整个数据集中：
```{r}
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 2)
summary(pls.fit)
```

两个PLS成分解释的变量Salary的方差百分比为46.4%，与选择的7个成分的PCR解释的46.69%基本相当。因为PCR旨在寻找一个可以解释预测变量方差最大化，而PLS旨在寻找一个可以同时解释预测变量和响应变量方差的方向。
```{r}
names(pls.fit)
coef(pls.fit)
```

